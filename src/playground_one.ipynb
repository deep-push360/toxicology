{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPARISON OF MOVIE REVIEW DATA WITH TOXIC DATA\n",
    "\n",
    "comment_movie_pos_file = '/home/kayode/KAYODE/PROJECTS/cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.pos'\n",
    "comment_movie_neg_file = '/home/kayode/KAYODE/PROJECTS/cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.neg'\n",
    "\n",
    "toxic_train_data_file = '/home/kayode/KAYODE/PROJECTS/Toxic_Kaggle/train.csv'\n",
    "toxic_test_data_file = '/home/kayode/KAYODE/PROJECTS/Toxic_Kaggle/test.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from preprocessing import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5c8ee036292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# add new column: Clean = 1 if all others are 0s and 0 otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m data['clean'] = np.where((data['toxic']==0) & (data['insult']==0) & (data['identity_hate']==0) \n\u001b[0;32m----> 3\u001b[0;31m              & (data['obscene']==0) & (data['severe_toxic']==0) & (data['threat']==0), 1, 0)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# add new column: Clean = 1 if all others are 0s and 0 otherwise \n",
    "data['clean'] = np.where((data['toxic']==0) & (data['insult']==0) & (data['identity_hate']==0) \n",
    "             & (data['obscene']==0) & (data['severe_toxic']==0) & (data['threat']==0), 1, 0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(dataset_file_name, comment_header, annotation_index_start):\n",
    "    data = pd.read_csv(dataset_file_name)\n",
    "    comments = [d for d in data[comment_header]]\n",
    "    comments = [clean_str(str(comments[3:9])) for comment in comments[3:9]]\n",
    "    #print(comments[3:9])\n",
    "    annotation = np.array([d for d in data.apply(lambda s: list(s[annotation_index_start:]), axis=1)])\n",
    "    return [comments, annotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment_train, label_train = data_generator(toxic_train_data_file, 'comment_text', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_examples =list(open(comment_movie_pos_file, \"r\").readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_examples = list(open(comment_movie_neg_file, \"r\").readlines())\n",
    "negative_examples = [s.strip() for s in negative_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    Input: positive data file and negative data file\n",
    "    Output: List of words and corresponding labels\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = load_data_and_labels(comment_movie_pos_file, comment_movie_neg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read csv file as text file\n",
    "\n",
    "# with open(toxic_train_data_file) as input_file:\n",
    "#     lines = [line.split(\",\", 2) for line in input_file.readlines()]\n",
    "#     text_list = [\" \".join(line) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model for NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    Arguments:\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            print(self.embedded_chars.shape)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            print(self.embedded_chars_expanded.shape)\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "(?, 56, 128)\n",
      "(?, 56, 128, 1)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/kayode/KAYODE/PROJECTS/DEEPPUSH_COLLARKAY/toxicology/src/runs/1516236932\n",
      "\n",
      "2018-01-18T02:55:33.532236: step 1, loss 1.80836, acc 0.5625\n",
      "2018-01-18T02:55:33.793366: step 2, loss 2.08279, acc 0.515625\n",
      "2018-01-18T02:55:34.032335: step 3, loss 1.84598, acc 0.578125\n",
      "2018-01-18T02:55:34.290650: step 4, loss 2.45031, acc 0.484375\n",
      "2018-01-18T02:55:34.475976: step 5, loss 1.71934, acc 0.5\n",
      "2018-01-18T02:55:34.654086: step 6, loss 2.02882, acc 0.46875\n",
      "2018-01-18T02:55:34.810968: step 7, loss 1.94426, acc 0.46875\n",
      "2018-01-18T02:55:34.993141: step 8, loss 1.47052, acc 0.53125\n",
      "2018-01-18T02:55:35.149916: step 9, loss 2.29659, acc 0.421875\n",
      "2018-01-18T02:55:35.325751: step 10, loss 1.6398, acc 0.546875\n",
      "2018-01-18T02:55:35.486289: step 11, loss 2.27347, acc 0.53125\n",
      "2018-01-18T02:55:35.658685: step 12, loss 2.37763, acc 0.4375\n",
      "2018-01-18T02:55:35.814449: step 13, loss 2.21242, acc 0.46875\n",
      "2018-01-18T02:55:35.982455: step 14, loss 1.83259, acc 0.53125\n",
      "2018-01-18T02:55:36.133584: step 15, loss 2.27115, acc 0.421875\n",
      "2018-01-18T02:55:36.306018: step 16, loss 1.88136, acc 0.5\n",
      "2018-01-18T02:55:36.464963: step 17, loss 1.48091, acc 0.5\n",
      "2018-01-18T02:55:36.680674: step 18, loss 1.77574, acc 0.5\n",
      "2018-01-18T02:55:36.923469: step 19, loss 1.93217, acc 0.5\n",
      "2018-01-18T02:55:37.180476: step 20, loss 1.52876, acc 0.5625\n",
      "2018-01-18T02:55:37.408459: step 21, loss 1.73218, acc 0.578125\n",
      "2018-01-18T02:55:37.643077: step 22, loss 2.09154, acc 0.5\n",
      "2018-01-18T02:55:37.807162: step 23, loss 2.21784, acc 0.421875\n",
      "2018-01-18T02:55:37.987279: step 24, loss 1.71612, acc 0.546875\n",
      "2018-01-18T02:55:38.142006: step 25, loss 1.84948, acc 0.484375\n",
      "2018-01-18T02:55:38.317459: step 26, loss 1.72118, acc 0.59375\n",
      "2018-01-18T02:55:38.475913: step 27, loss 1.68036, acc 0.5625\n",
      "2018-01-18T02:55:38.654620: step 28, loss 1.74875, acc 0.46875\n",
      "2018-01-18T02:55:38.815817: step 29, loss 1.60734, acc 0.46875\n",
      "2018-01-18T02:55:38.989480: step 30, loss 1.84953, acc 0.40625\n",
      "2018-01-18T02:55:39.141169: step 31, loss 1.45013, acc 0.5\n",
      "2018-01-18T02:55:39.320136: step 32, loss 1.73979, acc 0.46875\n",
      "2018-01-18T02:55:39.471561: step 33, loss 1.77227, acc 0.5\n",
      "2018-01-18T02:55:39.645395: step 34, loss 1.16904, acc 0.625\n",
      "2018-01-18T02:55:39.798848: step 35, loss 2.12866, acc 0.484375\n",
      "2018-01-18T02:55:39.978123: step 36, loss 1.51332, acc 0.5625\n",
      "2018-01-18T02:55:40.135585: step 37, loss 1.86395, acc 0.46875\n",
      "2018-01-18T02:55:40.311747: step 38, loss 1.46875, acc 0.53125\n",
      "2018-01-18T02:55:40.468304: step 39, loss 1.70901, acc 0.53125\n",
      "2018-01-18T02:55:40.639572: step 40, loss 1.80808, acc 0.46875\n",
      "2018-01-18T02:55:40.796362: step 41, loss 1.59925, acc 0.546875\n",
      "2018-01-18T02:55:40.972829: step 42, loss 1.76243, acc 0.453125\n",
      "2018-01-18T02:55:41.124603: step 43, loss 1.85702, acc 0.5\n",
      "2018-01-18T02:55:41.294706: step 44, loss 1.80105, acc 0.546875\n",
      "2018-01-18T02:55:41.450473: step 45, loss 1.87251, acc 0.484375\n",
      "2018-01-18T02:55:41.623674: step 46, loss 1.85134, acc 0.515625\n",
      "2018-01-18T02:55:41.776995: step 47, loss 1.76806, acc 0.484375\n",
      "2018-01-18T02:55:41.947580: step 48, loss 1.51942, acc 0.515625\n",
      "2018-01-18T02:55:42.101810: step 49, loss 1.70233, acc 0.484375\n",
      "2018-01-18T02:55:42.276481: step 50, loss 1.32094, acc 0.59375\n",
      "2018-01-18T02:55:42.431094: step 51, loss 1.65592, acc 0.5625\n",
      "2018-01-18T02:55:42.604701: step 52, loss 1.46324, acc 0.5625\n",
      "2018-01-18T02:55:42.757577: step 53, loss 1.5578, acc 0.53125\n",
      "2018-01-18T02:55:42.925874: step 54, loss 2.12384, acc 0.421875\n",
      "2018-01-18T02:55:43.077462: step 55, loss 1.3146, acc 0.5625\n",
      "2018-01-18T02:55:43.246575: step 56, loss 1.4542, acc 0.53125\n",
      "2018-01-18T02:55:43.402879: step 57, loss 1.73002, acc 0.53125\n",
      "2018-01-18T02:55:43.579878: step 58, loss 1.6252, acc 0.484375\n",
      "2018-01-18T02:55:43.731625: step 59, loss 1.67883, acc 0.4375\n",
      "2018-01-18T02:55:43.905684: step 60, loss 1.37535, acc 0.515625\n",
      "2018-01-18T02:55:44.057843: step 61, loss 1.43119, acc 0.578125\n",
      "2018-01-18T02:55:44.235638: step 62, loss 1.48624, acc 0.546875\n",
      "2018-01-18T02:55:44.386807: step 63, loss 1.67724, acc 0.515625\n",
      "2018-01-18T02:55:44.556316: step 64, loss 1.1423, acc 0.5625\n",
      "2018-01-18T02:55:44.715356: step 65, loss 1.63649, acc 0.53125\n",
      "2018-01-18T02:55:44.886798: step 66, loss 1.09135, acc 0.609375\n",
      "2018-01-18T02:55:45.040626: step 67, loss 1.65294, acc 0.5\n",
      "2018-01-18T02:55:45.213520: step 68, loss 1.46902, acc 0.59375\n",
      "2018-01-18T02:55:45.366584: step 69, loss 1.44627, acc 0.53125\n",
      "2018-01-18T02:55:45.537061: step 70, loss 1.95186, acc 0.5\n",
      "2018-01-18T02:55:45.692847: step 71, loss 1.34491, acc 0.515625\n",
      "2018-01-18T02:55:45.859805: step 72, loss 1.48232, acc 0.546875\n",
      "2018-01-18T02:55:46.016064: step 73, loss 1.50058, acc 0.65625\n",
      "2018-01-18T02:55:46.193332: step 74, loss 1.62682, acc 0.515625\n",
      "2018-01-18T02:55:46.344211: step 75, loss 1.9971, acc 0.421875\n",
      "2018-01-18T02:55:46.522098: step 76, loss 1.58879, acc 0.5\n",
      "2018-01-18T02:55:46.706510: step 77, loss 1.66974, acc 0.5\n",
      "2018-01-18T02:55:46.884549: step 78, loss 1.42753, acc 0.59375\n",
      "2018-01-18T02:55:47.045402: step 79, loss 1.25783, acc 0.609375\n",
      "2018-01-18T02:55:47.254645: step 80, loss 2.3244, acc 0.390625\n",
      "2018-01-18T02:55:47.470297: step 81, loss 1.28903, acc 0.578125\n",
      "2018-01-18T02:55:47.670518: step 82, loss 1.43457, acc 0.5625\n",
      "2018-01-18T02:55:47.850432: step 83, loss 1.10666, acc 0.609375\n",
      "2018-01-18T02:55:48.003770: step 84, loss 1.45468, acc 0.59375\n",
      "2018-01-18T02:55:48.196329: step 85, loss 1.12422, acc 0.625\n",
      "2018-01-18T02:55:48.437500: step 86, loss 1.45796, acc 0.5\n",
      "2018-01-18T02:55:48.684155: step 87, loss 1.41672, acc 0.5\n",
      "2018-01-18T02:55:48.869885: step 88, loss 1.7504, acc 0.4375\n",
      "2018-01-18T02:55:49.048151: step 89, loss 1.35176, acc 0.46875\n",
      "2018-01-18T02:55:49.284107: step 90, loss 1.28864, acc 0.46875\n",
      "2018-01-18T02:55:49.546709: step 91, loss 1.27351, acc 0.515625\n",
      "2018-01-18T02:55:49.769033: step 92, loss 0.947268, acc 0.65625\n",
      "2018-01-18T02:55:49.998092: step 93, loss 1.92837, acc 0.5625\n",
      "2018-01-18T02:55:50.199200: step 94, loss 1.08212, acc 0.6875\n",
      "2018-01-18T02:55:50.377876: step 95, loss 1.51846, acc 0.46875\n",
      "2018-01-18T02:55:50.565565: step 96, loss 1.54086, acc 0.578125\n",
      "2018-01-18T02:55:50.730617: step 97, loss 1.54655, acc 0.453125\n",
      "2018-01-18T02:55:50.891741: step 98, loss 1.20366, acc 0.546875\n",
      "2018-01-18T02:55:51.066015: step 99, loss 1.34763, acc 0.53125\n",
      "2018-01-18T02:55:51.374745: step 100, loss 1.29469, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2018-01-18T02:55:52.441012: step 100, loss 0.893951, acc 0.551595\n",
      "\n",
      "Saved model checkpoint to /home/kayode/KAYODE/PROJECTS/DEEPPUSH_COLLARKAY/toxicology/src/runs/1516236932/checkpoints/model-100\n",
      "\n",
      "2018-01-18T02:55:52.900478: step 101, loss 1.42467, acc 0.53125\n",
      "2018-01-18T02:55:53.065036: step 102, loss 1.22602, acc 0.59375\n",
      "2018-01-18T02:55:53.225898: step 103, loss 1.31178, acc 0.53125\n",
      "2018-01-18T02:55:53.421407: step 104, loss 1.4874, acc 0.5625\n",
      "2018-01-18T02:55:53.668531: step 105, loss 1.22794, acc 0.640625\n",
      "2018-01-18T02:55:53.831781: step 106, loss 1.46482, acc 0.53125\n",
      "2018-01-18T02:55:54.002925: step 107, loss 1.18688, acc 0.5625\n",
      "2018-01-18T02:55:54.158626: step 108, loss 1.387, acc 0.46875\n",
      "2018-01-18T02:55:54.331395: step 109, loss 1.47622, acc 0.5625\n",
      "2018-01-18T02:55:54.491937: step 110, loss 1.15848, acc 0.5625\n",
      "2018-01-18T02:55:54.669738: step 111, loss 1.11164, acc 0.640625\n",
      "2018-01-18T02:55:54.824500: step 112, loss 1.83672, acc 0.484375\n",
      "2018-01-18T02:55:55.002266: step 113, loss 1.17575, acc 0.578125\n",
      "2018-01-18T02:55:55.159705: step 114, loss 1.04005, acc 0.671875\n",
      "2018-01-18T02:55:55.328542: step 115, loss 1.34542, acc 0.546875\n",
      "2018-01-18T02:55:55.485992: step 116, loss 1.38719, acc 0.484375\n",
      "2018-01-18T02:55:55.656524: step 117, loss 1.55614, acc 0.546875\n",
      "2018-01-18T02:55:55.809343: step 118, loss 1.24638, acc 0.59375\n",
      "2018-01-18T02:55:55.981505: step 119, loss 1.25208, acc 0.515625\n",
      "2018-01-18T02:55:56.138638: step 120, loss 1.34174, acc 0.609375\n",
      "2018-01-18T02:55:56.309528: step 121, loss 1.28063, acc 0.609375\n",
      "2018-01-18T02:55:56.463966: step 122, loss 1.42138, acc 0.515625\n",
      "2018-01-18T02:55:56.636004: step 123, loss 1.47952, acc 0.546875\n",
      "2018-01-18T02:55:56.790837: step 124, loss 1.36341, acc 0.53125\n",
      "2018-01-18T02:55:56.960674: step 125, loss 1.36717, acc 0.578125\n",
      "2018-01-18T02:55:57.125129: step 126, loss 1.44442, acc 0.46875\n",
      "2018-01-18T02:55:57.302415: step 127, loss 1.47578, acc 0.4375\n",
      "2018-01-18T02:55:57.461751: step 128, loss 1.0109, acc 0.625\n",
      "2018-01-18T02:55:57.636023: step 129, loss 1.37829, acc 0.546875\n",
      "2018-01-18T02:55:57.797626: step 130, loss 1.08663, acc 0.578125\n",
      "2018-01-18T02:55:57.973174: step 131, loss 1.35387, acc 0.53125\n",
      "2018-01-18T02:55:58.130417: step 132, loss 1.16091, acc 0.546875\n",
      "2018-01-18T02:55:58.306432: step 133, loss 1.72461, acc 0.515625\n",
      "2018-01-18T02:55:58.463591: step 134, loss 1.2323, acc 0.5\n",
      "2018-01-18T02:55:58.739352: step 135, loss 1.05096, acc 0.59375\n",
      "2018-01-18T02:55:58.987366: step 136, loss 1.02394, acc 0.578125\n",
      "2018-01-18T02:55:59.269029: step 137, loss 1.11771, acc 0.59375\n",
      "2018-01-18T02:55:59.488773: step 138, loss 1.13344, acc 0.65625\n",
      "2018-01-18T02:55:59.653768: step 139, loss 1.15403, acc 0.578125\n",
      "2018-01-18T02:55:59.824129: step 140, loss 1.46498, acc 0.453125\n",
      "2018-01-18T02:56:00.031357: step 141, loss 1.0556, acc 0.671875\n",
      "2018-01-18T02:56:00.285144: step 142, loss 1.34569, acc 0.484375\n",
      "2018-01-18T02:56:00.444486: step 143, loss 1.26583, acc 0.546875\n",
      "2018-01-18T02:56:00.613264: step 144, loss 1.09859, acc 0.65625\n",
      "2018-01-18T02:56:00.820779: step 145, loss 1.44344, acc 0.453125\n",
      "2018-01-18T02:56:01.100149: step 146, loss 1.03244, acc 0.59375\n",
      "2018-01-18T02:56:01.372405: step 147, loss 1.06209, acc 0.578125\n",
      "2018-01-18T02:56:01.564933: step 148, loss 1.26077, acc 0.609375\n",
      "2018-01-18T02:56:01.743385: step 149, loss 0.801525, acc 0.671875\n",
      "2018-01-18T02:56:01.976024: step 150, loss 1.06773, acc 0.616667\n",
      "2018-01-18T02:56:02.272015: step 151, loss 0.968204, acc 0.640625\n",
      "2018-01-18T02:56:02.516576: step 152, loss 0.951286, acc 0.609375\n",
      "2018-01-18T02:56:02.685497: step 153, loss 0.956717, acc 0.609375\n",
      "2018-01-18T02:56:02.888687: step 154, loss 1.00294, acc 0.65625\n",
      "2018-01-18T02:56:03.176566: step 155, loss 0.769239, acc 0.703125\n",
      "2018-01-18T02:56:03.457784: step 156, loss 0.832882, acc 0.65625\n",
      "2018-01-18T02:56:03.716539: step 157, loss 0.693345, acc 0.6875\n",
      "2018-01-18T02:56:03.880403: step 158, loss 0.827474, acc 0.640625\n",
      "2018-01-18T02:56:04.052595: step 159, loss 0.953554, acc 0.578125\n",
      "2018-01-18T02:56:04.215022: step 160, loss 1.26807, acc 0.4375\n",
      "2018-01-18T02:56:04.387302: step 161, loss 0.823077, acc 0.640625\n",
      "2018-01-18T02:56:04.543087: step 162, loss 0.890236, acc 0.609375\n",
      "2018-01-18T02:56:04.723458: step 163, loss 1.11456, acc 0.5625\n",
      "2018-01-18T02:56:04.876989: step 164, loss 0.984906, acc 0.609375\n",
      "2018-01-18T02:56:05.054792: step 165, loss 0.831374, acc 0.609375\n",
      "2018-01-18T02:56:05.210609: step 166, loss 1.19254, acc 0.59375\n",
      "2018-01-18T02:56:05.379893: step 167, loss 0.861915, acc 0.640625\n",
      "2018-01-18T02:56:05.532926: step 168, loss 0.884714, acc 0.5625\n",
      "2018-01-18T02:56:05.706113: step 169, loss 1.30367, acc 0.5\n",
      "2018-01-18T02:56:05.861211: step 170, loss 1.07541, acc 0.65625\n",
      "2018-01-18T02:56:06.033456: step 171, loss 0.632031, acc 0.625\n",
      "2018-01-18T02:56:06.189893: step 172, loss 0.920077, acc 0.640625\n",
      "2018-01-18T02:56:06.367973: step 173, loss 0.94826, acc 0.59375\n",
      "2018-01-18T02:56:06.531343: step 174, loss 1.05601, acc 0.546875\n",
      "2018-01-18T02:56:06.715362: step 175, loss 0.761352, acc 0.75\n",
      "2018-01-18T02:56:06.880177: step 176, loss 0.848316, acc 0.625\n",
      "2018-01-18T02:56:07.065865: step 177, loss 0.837681, acc 0.625\n",
      "2018-01-18T02:56:07.240097: step 178, loss 0.895075, acc 0.640625\n",
      "2018-01-18T02:56:07.420174: step 179, loss 0.909907, acc 0.625\n",
      "2018-01-18T02:56:07.587196: step 180, loss 0.779403, acc 0.640625\n",
      "2018-01-18T02:56:07.769767: step 181, loss 0.881347, acc 0.59375\n",
      "2018-01-18T02:56:07.937421: step 182, loss 1.04738, acc 0.59375\n",
      "2018-01-18T02:56:08.118902: step 183, loss 0.969284, acc 0.515625\n",
      "2018-01-18T02:56:08.286876: step 184, loss 0.689257, acc 0.671875\n",
      "2018-01-18T02:56:08.467516: step 185, loss 1.09521, acc 0.515625\n",
      "2018-01-18T02:56:08.638106: step 186, loss 0.943675, acc 0.625\n",
      "2018-01-18T02:56:08.823179: step 187, loss 0.727512, acc 0.640625\n",
      "2018-01-18T02:56:08.991063: step 188, loss 1.30178, acc 0.484375\n",
      "2018-01-18T02:56:09.171796: step 189, loss 1.0726, acc 0.53125\n",
      "2018-01-18T02:56:09.338627: step 190, loss 1.06957, acc 0.5625\n",
      "2018-01-18T02:56:09.521067: step 191, loss 0.795208, acc 0.6875\n",
      "2018-01-18T02:56:09.686510: step 192, loss 1.1039, acc 0.578125\n",
      "2018-01-18T02:56:09.871430: step 193, loss 0.818999, acc 0.625\n",
      "2018-01-18T02:56:10.038229: step 194, loss 1.20801, acc 0.53125\n",
      "2018-01-18T02:56:10.226417: step 195, loss 0.932687, acc 0.625\n",
      "2018-01-18T02:56:10.390443: step 196, loss 1.00714, acc 0.578125\n",
      "2018-01-18T02:56:10.579017: step 197, loss 0.945422, acc 0.609375\n",
      "2018-01-18T02:56:10.752578: step 198, loss 1.25782, acc 0.5625\n",
      "2018-01-18T02:56:10.938442: step 199, loss 0.977084, acc 0.546875\n",
      "2018-01-18T02:56:11.106200: step 200, loss 1.14398, acc 0.53125\n",
      "\n",
      "Evaluation:\n",
      "2018-01-18T02:56:11.799159: step 200, loss 0.688821, acc 0.602251\n",
      "\n",
      "Saved model checkpoint to /home/kayode/KAYODE/PROJECTS/DEEPPUSH_COLLARKAY/toxicology/src/runs/1516236932/checkpoints/model-200\n",
      "\n",
      "2018-01-18T02:56:12.040992: step 201, loss 0.841499, acc 0.671875\n",
      "2018-01-18T02:56:12.319384: step 202, loss 0.746891, acc 0.6875\n",
      "2018-01-18T02:56:12.581254: step 203, loss 0.822932, acc 0.65625\n",
      "2018-01-18T02:56:12.753963: step 204, loss 0.808712, acc 0.65625\n",
      "2018-01-18T02:56:12.944183: step 205, loss 1.12742, acc 0.484375\n",
      "2018-01-18T02:56:13.116502: step 206, loss 0.8722, acc 0.65625\n",
      "2018-01-18T02:56:13.316680: step 207, loss 1.11063, acc 0.59375\n",
      "2018-01-18T02:56:13.492371: step 208, loss 1.17397, acc 0.625\n",
      "2018-01-18T02:56:13.690315: step 209, loss 1.04257, acc 0.59375\n",
      "2018-01-18T02:56:13.885677: step 210, loss 0.966558, acc 0.546875\n",
      "2018-01-18T02:56:14.113832: step 211, loss 1.00053, acc 0.59375\n",
      "2018-01-18T02:56:14.316698: step 212, loss 1.08227, acc 0.5625\n",
      "2018-01-18T02:56:14.545715: step 213, loss 0.839237, acc 0.578125\n",
      "2018-01-18T02:56:14.779678: step 214, loss 0.943956, acc 0.609375\n",
      "2018-01-18T02:56:14.997860: step 215, loss 0.720635, acc 0.671875\n",
      "2018-01-18T02:56:15.173562: step 216, loss 0.777766, acc 0.625\n",
      "2018-01-18T02:56:15.375566: step 217, loss 0.999891, acc 0.59375\n",
      "2018-01-18T02:56:15.544889: step 218, loss 1.01627, acc 0.640625\n",
      "2018-01-18T02:56:15.730362: step 219, loss 0.792531, acc 0.609375\n",
      "2018-01-18T02:56:15.893685: step 220, loss 0.629061, acc 0.65625\n",
      "2018-01-18T02:56:16.080272: step 221, loss 0.958939, acc 0.578125\n",
      "2018-01-18T02:56:16.249552: step 222, loss 0.671366, acc 0.578125\n",
      "2018-01-18T02:56:16.433740: step 223, loss 1.0179, acc 0.578125\n",
      "2018-01-18T02:56:16.610189: step 224, loss 1.00418, acc 0.640625\n",
      "2018-01-18T02:56:16.794106: step 225, loss 0.950599, acc 0.609375\n",
      "2018-01-18T02:56:16.962910: step 226, loss 0.677025, acc 0.671875\n",
      "2018-01-18T02:56:17.148161: step 227, loss 0.73483, acc 0.609375\n",
      "2018-01-18T02:56:17.319663: step 228, loss 1.1151, acc 0.484375\n",
      "2018-01-18T02:56:17.508258: step 229, loss 0.836076, acc 0.625\n",
      "2018-01-18T02:56:17.671481: step 230, loss 0.920027, acc 0.546875\n",
      "2018-01-18T02:56:17.869172: step 231, loss 1.13109, acc 0.53125\n",
      "2018-01-18T02:56:18.039105: step 232, loss 0.778118, acc 0.671875\n",
      "2018-01-18T02:56:18.226559: step 233, loss 0.941148, acc 0.5625\n",
      "2018-01-18T02:56:18.393911: step 234, loss 0.721316, acc 0.703125\n",
      "2018-01-18T02:56:18.585948: step 235, loss 0.634676, acc 0.71875\n",
      "2018-01-18T02:56:18.756968: step 236, loss 0.924705, acc 0.59375\n",
      "2018-01-18T02:56:18.938939: step 237, loss 0.92074, acc 0.59375\n",
      "2018-01-18T02:56:19.106147: step 238, loss 0.755554, acc 0.578125\n",
      "2018-01-18T02:56:19.289159: step 239, loss 1.07695, acc 0.453125\n",
      "2018-01-18T02:56:19.451851: step 240, loss 0.79037, acc 0.625\n",
      "2018-01-18T02:56:19.640147: step 241, loss 0.847465, acc 0.609375\n",
      "2018-01-18T02:56:19.804308: step 242, loss 0.628118, acc 0.71875\n",
      "2018-01-18T02:56:19.986341: step 243, loss 0.881657, acc 0.609375\n",
      "2018-01-18T02:56:20.150644: step 244, loss 0.943386, acc 0.59375\n",
      "2018-01-18T02:56:20.335154: step 245, loss 0.852365, acc 0.640625\n",
      "2018-01-18T02:56:20.499644: step 246, loss 1.06591, acc 0.515625\n",
      "2018-01-18T02:56:20.680084: step 247, loss 0.938551, acc 0.625\n",
      "2018-01-18T02:56:20.841958: step 248, loss 0.902301, acc 0.578125\n",
      "2018-01-18T02:56:21.029027: step 249, loss 0.882126, acc 0.5\n",
      "2018-01-18T02:56:21.195515: step 250, loss 0.742109, acc 0.625\n",
      "2018-01-18T02:56:21.378857: step 251, loss 0.743489, acc 0.609375\n",
      "2018-01-18T02:56:21.546811: step 252, loss 0.905755, acc 0.5\n",
      "2018-01-18T02:56:21.725863: step 253, loss 0.861501, acc 0.5625\n",
      "2018-01-18T02:56:21.890207: step 254, loss 0.771779, acc 0.609375\n",
      "2018-01-18T02:56:22.073373: step 255, loss 0.702049, acc 0.59375\n",
      "2018-01-18T02:56:22.237188: step 256, loss 0.782279, acc 0.609375\n",
      "2018-01-18T02:56:22.417292: step 257, loss 1.13127, acc 0.5\n",
      "2018-01-18T02:56:22.588148: step 258, loss 0.890188, acc 0.578125\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "#import data_helpers\n",
    "#from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"/home/kayode/KAYODE/PROJECTS/cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"/home/kayode/KAYODE/PROJECTS/cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "#FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The cross-entropy loss we are using to optimize the network measures the \"distance\" between\n",
    "two probability distributions. Our network outputs probabilities of the form [prob_positive,\n",
    "prob_negative] and the true labels are [100%, 0%] (positive) or [0%, 100%] (negative). \n",
    "That's why we are converting the labels to one-hot vectors, so we can use them with the \n",
    "cross-entropy loss. Hope that makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    #define the data directory where the templates live\n",
    "    data_dir = \"./data/templates/\"\n",
    "    data_dir = \"./data/rt-polaritydata/\"\n",
    "\n",
    "    #store all of the class data in a list\n",
    "    class_data = []\n",
    "    label_list = []\n",
    "    default_list = []\n",
    "\n",
    "    #load data from files\n",
    "    for i in os.listdir(data_dir):\n",
    "        print data_dir+i\n",
    "        examples = list(open(data_dir+i).readlines())\n",
    "        examples = [s.strip() for s in examples]\n",
    "        #append these examples to the list of lists\n",
    "        class_data.append(examples)\n",
    "        #make the label list as long as the numbe rof classes\n",
    "        default_list.append(0)\n",
    "\n",
    "    # concat class examples\n",
    "    counter = 0\n",
    "    for class_examples in class_data:\n",
    "        #set the label\n",
    "        temp_list = zerolist(class_data)\n",
    "        temp_list[counter] = 1\n",
    "        label_list.append(temp_list)\n",
    "        if counter == 0:\n",
    "            x_text = class_examples\n",
    "        else:\n",
    "            x_text = x_text + class_examples\n",
    "        counter += 1\n",
    "\n",
    "    #clean and split\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "\n",
    "    # Generate labels\n",
    "    final_labels = []\n",
    "    counter = 0\n",
    "    for class_examples in class_data:\n",
    "        print label_list[counter]\n",
    "        final_labels.append([label_list[counter] for _ in class_data[counter]])\n",
    "        counter += 1\n",
    "\n",
    "    y = np.concatenate(final_labels, 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate CNN Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
