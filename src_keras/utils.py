"""Contributors: Kayode Olaleye"""

import numpy as np
import re
import os
import errno
import pickle
import sys
from os.path import join
import pandas as pd
import keras.models
import itertools
from collections import Counter
from keras.preprocessing import text, sequence
from sklearn.model_selection import train_test_split
from preprocessing import data_generator, clean_punctuation
from config import OUTPUT_DIR, TENSORBOARD_DIR, MODELS_DIR
from config import SAMPLE_SUBMISSION_FILE

#LIST_OF_CLASSES = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

def create_directories():
    directories = [CACHE_DATA_DIR, MODELS_DIR, OUTPUT_DIR, TENSORBOARD_DIR]
    for directory in directories:
        save_makedirs(directory)

def save_makedirs(path):
    """ Create directory if and only if it does not exist yet"""
    try:
        os.makedirs(path)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise

def create_submission_file(list_of_classes, y_pred, model_id, output_dir):
    sample_submission = pd.read_csv(SAMPLE_SUBMISSION_FILE)
    sample_submission[list_of_classes] = y_pred
    DP_SUBMISSION_FILE = join(output_dir, model_id, "deeppush_toxic_classification.csv")
    sample_submission.to_csv(DP_SUBMISSION_FILE, index=False)
    return DP_SUBMISSION_FILE

# def create_submission_file_plus_one_class(list_of_classes, y_pred, model_id, output_dir):
#     sample_submission = pd.read_csv(SAMPLE_SUBMISSION_FILE)
#     #print(list_of_classes[:-1])
#     sample_submission[list_of_classes[:-1]] = y_pred[:,:-1]
#     DP_SUBMISSION_FILE = join(output_dir, model_id, "deeppush_toxic_classification.csv")
#     sample_submission.to_csv(DP_SUBMISSION_FILE, index=False)
#     return DP_SUBMISSION_FILE

# sample_submission = pd.read_csv("../data/input/sample_submission.csv")
# sample_submission[list_of_classes] = y_testing
# sample_submission.to_csv("../data/output/toxic_comment_classification.csv", index=False)    
    
def save_model_summary(hyperparameters, model, path):
    """Save hyperparameters of a model and the model summary generated by keras to a .txt file"""
    #Open the text file and write the hyparameter values to it
    with open(os.path.join(path, "hyperparameters.txt"), "w") as out:
        for parameter, value in hyperparameters:
            out.write("{}: {}\n".format(parameter, value))
            
        #model.summary() prints to stdout. copy the summary to out
        stdout = sys.stdout
        sys.stdout = out
        model.summary()
        sys.stdout = stdout

def save_model(model, path):
    """ Save a keras model and its weights at the given path. """
    
    print("Save trained model to {}.".format(path))
    model_path = os.path.join(path, "model.json")
    model_json = model.to_json()
    with open(model_path, "w") as json_file:
        json_file.write(model_json)
        
    weights_path = os.path.join(path, "weights.hdf5")
    model.save_weights(weights_path)
    
def load_model(model_id):
    """ Load a keras model and its weights with the given ID """
    
    model_dir = os.path.join(MODELS_DIR, model_id)
    
    print("Load model in {}.".format(model_dir))
    model_file = os.path.join(model_dir, "model.json")
    with open(model_file, "r") as f:
        json_file = f.read()
        model = keras.models.model_from_json(json_file)
        
    weights_file = os.path.join(model_dir, "weights.hdf5")
    model.load_weights(weights_file)
    
    return model        
            
def pad_comments(comments, padding_word="P"):
    """
    Pads all comments to the same length. The length is defined by the longest comment.
    Returns padded comments.
    """
    sequence_length = max(len(x) for x in comments)
    padded_comments = []
    for i in range(len(comments)):
        comment = comments[i]
        num_padding = sequence_length - len(comment)
        new_comment = comment + padding_word * num_padding
        padded_comments.append(new_comment)
    return padded_comments

def build_vocab(comments):
    """
    Builds a vocabulary mapping from word to index based on the comments.
    Returns vocabulary mapping and inverse vocabulary mapping.
    """
    # Build vocabulary
    word_counts = Counter(itertools.chain(*comments))
    # Mapping from index to word
    vocab_inv = [x[0] for x in word_counts.most_common()]
    vocab_inv = list(sorted(vocab_inv))
    # Mapping from word to index
    vocab = {x: i for i, x in enumerate(vocab_inv)}
    return [vocab, vocab_inv]

def generate_input_data(comments, classes, vocab):
    """
    Maps comments and classes to vectors based on a vocabulary.
    """
    x = np.array([[vocab[word] for word in comment] for comment in comments])
    y = np.array(classes)
    return [x, y]

def generate_train_data(path_to_file, list_of_classes, comment_col = 'comment_text'):
    train_df = pd.read_csv(path_to_file)
#     train_df['clean'] = np.where(
#         (train_df['toxic']==0) & (train_df['insult']==0) & 
#         (train_df['identity_hate']==0) &(train_df['obscene']==0) &
#         (train_df['severe_toxic']==0) & (train_df['threat']==0), 1, 0)  
    #print(train_df.head())
    x = train_df[comment_col].fillna('comment_missing').values
    y = train_df[list_of_classes].values
    return x, y

def generate_test_data(path_to_file, comment_col = 'comment_text'):
    test_df = pd.read_csv(path_to_file)
    x_test = test_df[comment_col].fillna('comment_missing').values
    return x_test

def preprocess_test_data(path_to_file, vocab_size=1000,  sequence_length=400):
    x_test = generate_test_data(path_to_file)
    x_tokenizer = text.Tokenizer(num_words=vocab_size)
    x_tokenizer.fit_on_texts(list(x_test))
    x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)
    x_testing = sequence.pad_sequences(x_test_tokenized, maxlen=sequence_length)
    return x_testing

def preprocess_train_data(path_to_file, list_of_classes, vocab_size=1000, sequence_length=400):
    x, y = generate_train_data(path_to_file, list_of_classes)
    x_tokenizer = text.Tokenizer(num_words=vocab_size)
    x_tokenizer.fit_on_texts(list(x))
    x_tokenized = x_tokenizer.texts_to_sequences(x) 
    x_train_val = sequence.pad_sequences(x_tokenized, maxlen=sequence_length)
    return x_train_val, y, x_tokenizer

def get_coefs(word,*arr): 
    return word, np.asarray(arr, dtype='float32')

# Read the glove word vectors (space delimited strings) into a dictionary from word->vector.
def get_glove_embeddings(embedding_file): 
    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))
    return embeddings_index

"""Use these vectors to create our embedding matrix, with random initialization
for words that aren't in GloVe. We'll use the same mean and stdev of embeddings 
the GloVe has when generating the random init."""
def generate_emb_matrix(embedding_file, path_to_train_file, embedding_dims,
                        list_of_classes, vocab_size=1000, sequence_length=400):
    embeddings_index = get_glove_embeddings(embedding_file)
    all_embs = np.stack(embeddings_index.values())
    emb_mean,emb_std = all_embs.mean(), all_embs.std()
    _,_,tokenizer = preprocess_train_data(path_to_train_file, list_of_classes)
    word_index = tokenizer.word_index
    nb_words = min(vocab_size, len(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_dims))
    for word, i in word_index.items():
        if i >= vocab_size: continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None: embedding_matrix[i] = embedding_vector
        
    return embedding_matrix
        
def load_train_data(path_to_file, list_of_classes, sequence_length=400):
    """
    Loads and preprocessed data for the dataset.
"""
    print('_' * 100)
    print("Start preprocessing data.")
    
    
    # Load and preprocess data
    x_train_val, y, _ = preprocess_train_data(path_to_file, list_of_classes, sequence_length)
    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.1, random_state=1)
    return x_train, x_val, y_train, y_val

def load_test_data(path_to_file, sequence_length=400):
    x_testing = preprocess_test_data(path_to_file, sequence_length)
    return x_testing

def save_train_data(file_path, comments, labels, vocab, vocab_inv):
    """ Save the data as a pickle """
    
    print("Store preprocessed data at {}.".format(file_path))
    with open(file_path, "wb") as out:
        pickle.dump({"comments": comments, "labels": labels, "vocab": vocab,
                    "vocab_inv": vocab}, out)

def save_test_data(file_path, comments):
    """ Save the data as a pickle """
    
    print("Store preprocessed data at {}.".format(file_path))
    with open(file_path, "wb") as out:
        pickle.dump({"comments": comments}, out)



def batch_iter(data, batch_size, num_epochs, shuffle=True):
    """
    Generates a batch iterator for a dataset.
    """
    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1
    for epoch in range(num_epochs):
        # Shuffle the data at each epoch
        if shuffle:
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_index:end_index]