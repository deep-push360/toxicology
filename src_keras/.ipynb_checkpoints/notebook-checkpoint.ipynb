{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d493a15b-c3f8-4fc8-b6f6-b244262cc9cc",
    "_uuid": "f5e86b7bf4f117737ae32952c22d6ada16b45b5c"
   },
   "source": [
    "**Perform the necessary imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5e3d88fb-79b6-4876-aa4f-700ee70b6a97",
    "_uuid": "68cb2423a6f01d469434af0430887b537efcfb51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalMaxPooling2D, Conv2D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "de245cc0-77b6-41e1-95eb-a1c262a67d13",
    "_uuid": "36fe75d8b240981c941a1f0c67831acbb5b5b944"
   },
   "source": [
    "**Necessary global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9ee07ff6-a9f6-43e6-acd5-0c0b6991434c",
    "_uuid": "2b0d3e0df7b39c957a5aa82929828c6d38ed045c"
   },
   "outputs": [],
   "source": [
    "list_of_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "max_features = 20000\n",
    "max_text_length = 400\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "keepprob=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "297811ae-23ee-471a-b3c1-a39afda0603b",
    "_uuid": "f7c54e5252d6eaa902043414368ecf077a7122c4"
   },
   "source": [
    "**Quick peek into the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d30f5899-23a4-42a1-aa44-bb0f4e075104",
    "_uuid": "c7ce7a4f881105d1d64fe22b9197311e057696e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/input/train.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "672807f9-6ce2-47d1-9a1c-80fe07576b09",
    "_uuid": "4cae0fb90301fc9a5ed4005b498261eb939c213e"
   },
   "source": [
    "**Printing using 'iloc' just for fun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2b90735a-eb0a-4ee3-9545-2233f2fdee02",
    "_uuid": "25e62320d15cacbc201714c861591c18d2acc220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "print(train_df.iloc[0, -7])\n",
    "print(train_df.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "858757df-0cb8-4187-8c3d-1172ba673af4",
    "_uuid": "d107221dc682c12682832224bfe5cc6fd10462a9"
   },
   "source": [
    "**Checking if  NaNs exist in the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3a1ffb98-08b4-4363-b19f-d4b4ed6c808a",
    "_uuid": "8a9fcc9505de340c9f0a157b2c9aef37851e7b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(pd.isnull(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "752260bc-592e-40c1-8d4a-728873fdbe47",
    "_uuid": "6e3afb7d148fcba97b16dfa69898ede9d83edf01"
   },
   "source": [
    "**Apparently no NaNs in the training set!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4b1145e3-caa3-474d-b471-0fa37f402801",
    "_uuid": "0a76a762fce842e1d254d84d44af56cd6d33333e"
   },
   "source": [
    "**Converting pandas series to a numpy array using .values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a74097c1-ba58-45c1-ae57-551496a96994",
    "_uuid": "aaa6ccda53c3facedcd6f92e5e5f7408c6a28efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\"\n",
      " \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\"\n",
      " \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\"\n",
      " ...,\n",
      " 'Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.'\n",
      " 'And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.'\n",
      " '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"']\n"
     ]
    }
   ],
   "source": [
    "x = train_df['comment_text'].values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8263e42d-a76b-4e3b-bfb1-c42fe366f7bb",
    "_uuid": "bb2b3f733ffee73d9504542814ceda82282f7007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of x\n",
      "type : <class 'numpy.ndarray'>, dimensions : 1, shape : (159571,), total no. of elements : 159571, data type of each element: object, size of each element 8 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of x\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(x), x.ndim, x.shape, x.size, x.dtype, x.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88890c4f-bfd1-47f4-bc69-30435001c6f5",
    "_uuid": "7abe952d6a9b178edadc0751795649842c4ff042"
   },
   "source": [
    "**Getting the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "706f8df5-41b1-4a75-9d20-864b5cf3d791",
    "_uuid": "7b65845c54b983d2602e68f1e0438b9c712dc300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ..., \n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "y = train_df[list_of_classes].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "1a7180e9-0756-4f58-bb57-d37dde2bd528",
    "_uuid": "5fc08696b61be737b0b1f16b412abed4c5a2abb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of y\n",
      "type : <class 'numpy.ndarray'>, dimensions : 2, shape : (159571, 6), total no. of elements : 957426, data type of each element: int64, size of each element 8 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of y\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(y), y.ndim, y.shape, y.size, y.dtype, y.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af34631e-ff2c-4cf3-bee5-0b5a9bdb0db9",
    "_uuid": "b8ba64cd620fc3453bce297105fcdb8d6fb8e306"
   },
   "source": [
    "**Keras makes our life easy. Using Tokenizer to get a list of sequence and then padding it form a 2D numpy array **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "55cf2aa3-b53e-4f4e-b9f1-f4043f04120d",
    "_uuid": "e8ed34f540de09e56dcd95bc0e509d4a406eb8a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.text.Tokenizer object at 0x2aab2218e9e8>\n",
      "<keras.preprocessing.text.Tokenizer object at 0x2aab2218e9e8>\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print(x_tokenizer)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "print(x_tokenizer)\n",
    "x_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n",
    "#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \n",
    "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "758e64cc-2646-45bf-b9e1-ace946e8174b",
    "_uuid": "6ad16f7b64b4cd930ba3d2837937d9ad9548bb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "properties of x_train_val\n",
      "type : <class 'numpy.ndarray'>, dimensions : 2, shape : (159571, 400), total no. of elements : 63828400, data type of each element: int32, size of each element 4 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"properties of x_train_val\")\n",
    "print(\"type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes\".format(type(x_train_val), x_train_val.ndim, x_train_val.shape, x_train_val.size, x_train_val.dtype, x_train_val.itemsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd3693ba-3872-4e07-96f9-99087ae5bd70",
    "_uuid": "d68eea53d70a1701ba939991fdeb9976c8df05c4"
   },
   "source": [
    "**90% of the data is used for training and the rest for validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d84418e1-95e7-4024-b494-825e00ce8172",
    "_uuid": "4bdd0b3298393be4f5150c725569dadb789fb3b4"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a044595d-de8f-4d4d-b547-5c1fee4e4e4f",
    "_uuid": "2848de4a72693329cdd114c3e2b0d5fe2e5c4cd3"
   },
   "source": [
    "**Start building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "c66cbdb4-9c9d-4120-aed9-023f2d5c1659",
    "_uuid": "56417e8ca623249048c89be6ba75b18209abb6da"
   },
   "outputs": [],
   "source": [
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=max_text_length))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto 6 output layers, and squash it with a sigmoid:\n",
    "# model.add(Dense(6))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 400, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 400, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 398, 1, 250)       37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1506      \n",
      "=================================================================\n",
      "Total params: 1,102,006\n",
      "Trainable params: 1,102,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_text_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=max_features, \n",
    "                      output_dim=embedding_dims, input_length=max_text_length)(inputs)\n",
    "reshape = Reshape((max_text_length,embedding_dims,1))(embedding)\n",
    "drop = Dropout(keepprob)(reshape)\n",
    "conv_1 = Conv2D(filters, kernel_size=(kernel_size, embedding_dims), \n",
    "                padding='valid', kernel_initializer='normal', activation='relu')(drop)\n",
    "maxpool = GlobalMaxPooling2D()(conv_1)\n",
    "#conv_2 = Conv2D(num_filter_2, kernel_size=(filter_size_2, embedding_dim), \n",
    " #               padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "#conv_3 = Conv2D(num_filter_3, kernel_size=(filter_size_3, embedding_dim), \n",
    "  #              padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "#maxpool_1 = MaxPooling2D(pool_size=(sequence_length - filter_size_1 + 1, 1), \n",
    " #                     strides=(pool_stride_1, pool_stride_1), padding='valid')(conv_1)\n",
    "#maxpool_2 = MaxPooling2D(pool_size=(sequence_length - filter_size_2 + 1, 1), \n",
    " #                     strides=(pool_stride_2, pool_stride_2), padding='valid')(conv_2)\n",
    "#maxpool_3 = MaxPooling2D(pool_size=(sequence_length - filter_size_3 + 1, 1), \n",
    "#                      strides=(pool_stride_3, pool_stride_3), padding='valid')(conv_3)\n",
    "\n",
    "#concatenated_tensor = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])\n",
    "#flatten = Flatten()(maxpool)\n",
    "dense_1 = Dense(units=hidden_dims, activation='relu')(maxpool)\n",
    "dropout = Dropout(keepprob)(dense_1)\n",
    "output = Dense(units=6, activation='softmax')(dropout)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67a54a4c-f08a-490c-ae9f-e7a13c7e41e1",
    "_uuid": "b2aabd44e7161e983ddea65a6489fb9b18e187e9"
   },
   "source": [
    "**Begin training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "5ce38563-9ce9-4c5c-b084-350a5a74bc0b",
    "_uuid": "297008f72969696c9031cbdfa7eb2ed964a2069a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143613/143613 [==============================] - 223s 2ms/step - loss: 0.2223 - acc: 0.9643 - val_loss: 0.2200 - val_acc: 0.9634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aab251446d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb35da05-2661-42d2-be40-6656eda6ff71",
    "_uuid": "3592c6fa0c6100d8847db8ab7c53a9b85d6a48b1"
   },
   "source": [
    "\n",
    "**Quick peek into the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "f99b26ea-0ee9-4ea6-aa71-8e4e3a3d5cda",
    "_uuid": "038b1acf1b88e71ef0b5cd29070fd9bfa90c9c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
      "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
      "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
      "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
      "4  00017695ad8997eb          I don't anonymously edit articles at all.\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../data/input/test.csv')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96fe1fc4-cbc6-45df-98f9-79fa8fa9326f",
    "_uuid": "1b903303556d9efa8716153b453f5c19860c00e3"
   },
   "source": [
    "**Checking if  NaNs exist in the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "4ee9acc5-131f-4aee-b42d-7dba412338ee",
    "_uuid": "441dab5b1c9a842943d6d0f37537ae79b12e5868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(pd.isnull(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "219da36e-9c01-4cc5-bb37-b60dcf828e1d",
    "_uuid": "17003afb11c9a45651e950a999410d40dad13e10"
   },
   "source": [
    "**Hmmm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "0b578f37-57ea-4163-9544-7f98c51e5c43",
    "_uuid": "fac1694d92d39700287ea83c2f56aa1d38eb5315"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOmebody fucked up the homepage plz edit!! thanks, I need medevil knawledge.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[52300, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0bb52e9e-209c-44a3-b798-e03498a8fc54",
    "_uuid": "1d8461f41d14fe71577fa337d0666232181d5b9c"
   },
   "source": [
    "**Fill the NaN field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "f940ba4b-0381-4662-8c2d-ad2c4684dd14",
    "_uuid": "8ab3002d00c86fda5802a7546d722b62fc3bcfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"\n",
      " '== From RfC == \\n\\n The title is fine as it is, IMO.'\n",
      " '\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"' ...,\n",
      " '\" \\n\\n == Okinotorishima categories == \\n\\n I see your changes and agree this is \"\"more correct.\"\"  I had gotten confused, but then found this: \\n :... while acknowledging Japan\\'s territorial rights to Okinotorishima itself ... \\n However, is there a category for  \\n :... did not acknowledge Japan\\'s claim to an exclusive economic zone (EEZ) stemming from Okinotorishima. \\n That is, is there a category for \"\"disputed EEZ\"\"s?   \"'\n",
      " '\" \\n\\n == \"\"One of the founding nations of the EU - Germany - has a Law of Return quite similar to Israel\\'s\"\" == \\n\\n This isn\\'t actually true, is it? Germany allows people whose ancestors were citizens of Germany to return, but AFAIK it does not allow the descendants of Anglo-Saxons to \"\"return\"\" to Angeln and Saxony. Israel, by contrast, allows all Jews to \"\"return\"\" to Israel, even if they can\\'t trace a particular ancestral line to anyone who lived in the modern state or even mandate Palestine. — \"'\n",
      " '\" \\n :::Stop already. Your bullshit is not welcome here. I\\'m no fool, and if you think that kind of explination is enough, well pity you.    \"']\n"
     ]
    }
   ],
   "source": [
    "x_test = test_df['comment_text'].fillna('comment_missing').values\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3072aca7-6fd9-4da3-8561-730b9b6b1706",
    "_uuid": "f3b50a49e1a5dbf25ce0cb182db3da60a9db85d9"
   },
   "source": [
    "**Tokenizing and padding similar to what we did before to training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "67e8eb4e-7457-4649-9513-411794e55ada",
    "_uuid": "6c2b27f7d2c0c1f6158b52b965479f1e257a3afc"
   },
   "outputs": [],
   "source": [
    "x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\n",
    "x_testing = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ca42dea-b68c-4e31-a3f9-99e415ddc3be",
    "_uuid": "cadb4f9b8a646599eca8d107a2416214b49442ee"
   },
   "source": [
    "**Time to predict!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "d1cc833e-46f4-46bf-bbe0-0a39ef80ff5d",
    "_uuid": "d323d8503236d4cf01a3253f6639445ec9735ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 28s 186us/step\n"
     ]
    }
   ],
   "source": [
    "y_testing = model.predict(x_testing, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e25db199-5b05-455f-8e87-b2fd4b7265a0",
    "_uuid": "e3ec631b8e942bb6a3dd843f40d42241f1373308"
   },
   "source": [
    "**Submit predictions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "612d0744-ec1c-40bc-bed6-5028f9936ac6",
    "_uuid": "d87c22acba7dda6524eba4b7fdc6c915b8301174"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../data/input/sample_submission.csv\")\n",
    "sample_submission[list_of_classes] = y_testing\n",
    "sample_submission.to_csv(\"../data/output/toxic_comment_classification.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.344461</td>\n",
       "      <td>0.090538</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.306264</td>\n",
       "      <td>0.046379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.165703</td>\n",
       "      <td>0.172490</td>\n",
       "      <td>0.161471</td>\n",
       "      <td>0.169440</td>\n",
       "      <td>0.160765</td>\n",
       "      <td>0.170131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.195152</td>\n",
       "      <td>0.161928</td>\n",
       "      <td>0.161874</td>\n",
       "      <td>0.160031</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.158671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.134783</td>\n",
       "      <td>0.178342</td>\n",
       "      <td>0.175583</td>\n",
       "      <td>0.168711</td>\n",
       "      <td>0.170842</td>\n",
       "      <td>0.171739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.206071</td>\n",
       "      <td>0.159492</td>\n",
       "      <td>0.161634</td>\n",
       "      <td>0.156599</td>\n",
       "      <td>0.161650</td>\n",
       "      <td>0.154554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.344461      0.090538  0.205128  0.007230  0.306264   \n",
       "1  0000247867823ef7  0.165703      0.172490  0.161471  0.169440  0.160765   \n",
       "2  00013b17ad220c46  0.195152      0.161928  0.161874  0.160031  0.162344   \n",
       "3  00017563c3f7919a  0.134783      0.178342  0.175583  0.168711  0.170842   \n",
       "4  00017695ad8997eb  0.206071      0.159492  0.161634  0.156599  0.161650   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.046379  \n",
       "1       0.170131  \n",
       "2       0.158671  \n",
       "3       0.171739  \n",
       "4       0.154554  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
